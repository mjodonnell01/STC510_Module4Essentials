{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8304172-7124-40d5-bd93-b4d8ae6f9561",
   "metadata": {},
   "source": [
    "Module 4 Essentials Project\n",
    "\n",
    "Michael O'Donnell 02/01/2024\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "For this assignment, I will be using Selenium and Beautiful soup to scrape Gab for posts that disucuss the keyword 'Taylor Swift' since Gab is used by right-wingers and right-wingers are targeting Taylor with conspiracy theorie. Here is an article discussing this:\n",
    "\n",
    "https://www.msn.com/en-us/sports/nfl/right-wing-media-figures-target-taylor-swift-with-absurd-conspiracy-theory-ahead-of-the-super-bowl/ar-BB1htK8Q\n",
    "\n",
    "Before this conspiracy becomes irrelevant and is replaced by another conspiracy, probably against her sadly, I want to scrape and archive what the users of the platform are saying about her and try to gain some insight into the reasonings behind her being involved in a conspiracy theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b746f5c-a5a5-4b55-816d-dd79f73d0527",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "I used many resources for this assignment along with some handy dandy help from ChatGPT when I was stuck in the mud.\n",
    "\n",
    "Below are some resources that I used when coding this assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20265fa5-e777-4f98-8922-0cd30011aedc",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/43835404/importing-keys-from-selenium-webdriver-common-keys\n",
    "\n",
    "https://stackoverflow.com/questions/20986631/how-can-i-scroll-a-web-page-using-selenium-webdriver-in-python\n",
    "\n",
    "https://stackoverflow.com/questions/7271385/how-do-i-combine-two-lists-into-a-dictionary-in-python\n",
    "\n",
    "https://www.geeksforgeeks.org/split-a-text-column-into-two-columns-in-pandas-dataframe/#\n",
    "\n",
    "https://stackoverflow.com/questions/53141240/pandas-how-to-swap-or-reorder-columns\n",
    "\n",
    "https://www.easytweaks.com/add-column-current-date-timestamp-pandas/\n",
    "\n",
    "https://www.geeksforgeeks.org/how-to-rename-columns-in-pandas-dataframe/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "0505cc63-e74f-4cb5-8149-e613282c6859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "537ec5a6-11d0-422b-843c-3498e1f7ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Launches Chrome that I can manipulate\n",
    "driver=webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "014be3c0-c6db-4fc9-a06b-4b9a25f96675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goes to Gab\n",
    "driver.get('https://gab.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "ee8465e0-e299-40f7-9d39-f24a992ddeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This accesses the search and searches for Taylor Swift\n",
    "time.sleep(5)\n",
    "driver.find_element(By.ID,'nav-search').send_keys('Taylor Swift' + Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "b4816de3-2e0f-4fa4-b20f-7ae44fe7a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This scrolls twice to load in the first set of posts. For some reason Gab only pulls a preview for the first intitial load.\n",
    "time.sleep(5)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(5)\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#This then adds the source code to a variable to store it\n",
    "pagesource += driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "129debbe-559f-4f0f-b31f-2246cf7ee1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This for loop takes the above logic and then scrolls to the bottom of the page 5 times while adding additional code to the variable\n",
    "for i in range(0,4):\n",
    "    time.sleep(5)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    pagesource += driver.page_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a5ba37-e660-4214-bab1-683baed5fc0c",
   "metadata": {},
   "source": [
    "I noticed here that around 5 or so additional scrolls from the time I load in goes to about 2 hours worth of posts. So if I wanted this to run continually, I think I would run this every couple hours and have a way to make sure at the bottom that duplicated posts are not being added to the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "9bf083f7-3c66-4ee1-9f45-362490a1c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we have the HTML we want to look over stored in a variable, we no longer need the website up and running. \n",
    "time.sleep(10)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "827fe884-5341-4f3b-b849-7e7ada458ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This puts the HTML we gathered from the Selenium variable into a beautiful soup variable and parses it with HTML\n",
    "bs = BeautifulSoup(pagesource,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "d84a5592-5938-402f-a579-f445b3f5dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This finds the overarching div that contains the profile name and the body of text in the post\n",
    "meta_data_posts = bs.find_all('div', class_='_81_1w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "cf47a53c-e2e9-4dff-a633-2b0dc5693f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This sets two list variables that are going to hold the names and texts once they are delimitted\n",
    "post_texts = []\n",
    "author_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "6df637a1-2c48-4517-9e1e-3de335661f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This chunk of code loops through all of the beautiful soup variable/selenium variable with the html code grabbed from Gab\n",
    "#that then stores the author and text posts in variables. Those variables are then stored in corresponding lists if they match\n",
    "#each other from the metadata, so the posts and authors don't get mixed out of order\n",
    "for post in meta_data_posts:\n",
    "    post_text = post.find('div',class_='_1FwZr _81_1w')\n",
    "    author = post.find('div', class_='_UuSG Naf1t _3dGg1 ALevz')\n",
    "\n",
    "    if post_text and author:\n",
    "        post_texts.append(post_text.get_text(strip=True))\n",
    "        author_names.append(author.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "47a04cb9-c1eb-42d4-ad89-49b93f77d3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1330\n",
      "1330\n"
     ]
    }
   ],
   "source": [
    "#Checks to make sure that the columns will be the same length\n",
    "print(len(post_texts))\n",
    "print(len(author_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "6f364df1-0785-4b54-86c4-e24532298bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a new dataframe with the lists of names and posts\n",
    "df = pd.DataFrame({\n",
    "    'Author Name':author_names,\n",
    "    'Post Text':post_texts           \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "7ea07a0f-a4e0-452a-ad05-0acaf03cb4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this drops duplicate rows\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "2f273d31-2086-4203-919c-98d52e2625ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This drops rows with empty cells\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "632ac9c8-078a-4320-9bcc-d6d9eafefea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checks the max number of splits that will be occuring\n",
    "max_splits = df['Author Name'].str.count('@').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "dd06bc81-6002-4cae-8a75-25f184f5ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the author with the max number of @s so we can see the account name and account handle seperately\n",
    "split_cols = df['Author Name'].str.split('@', n=max_splits, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "d45e89f7-19ec-4d22-a182-47cbda3d5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Makes a new dataframe with the dataframe of split columns\n",
    "new_df = pd.concat([df, split_cols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "440f67e1-dbf5-46ff-a2a1-b9afa269df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since the posts don't include really include a time when posted, I added a column to show when these posts were grabbed\n",
    "#by the crawler. And in theory, since this would be running every 2 hours or so, this would be sort of accurate. It would\n",
    "#at least be pretty accurate on the date, but not the time. \n",
    "new_df['Post Grabbed Time'] = pd.Timestamp('now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "08dab582-f567-40b1-91d6-74816776b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drops a column we do not need\n",
    "new_df = new_df.drop(2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "5d308051-0931-4b4d-b53f-b1b879e6172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finds the number of verified accounts\n",
    "max_splits2 = new_df[0].str.count('Verified Account').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "f8eaf785-17f7-4c7a-80bd-47c2d8364f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the accounts where Verified Account is found in the author name\n",
    "split_cols2 = new_df[0].str.split('Verified Account', n=max_splits2, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "10eeb940-f1aa-441c-8615-acfb9857c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renames dataframe columns of columns that were split to make more sense\n",
    "split_cols2.rename(columns = {0:'Account Name'}, inplace = True)\n",
    "split_cols2.rename(columns = {1:'Verified Account'}, inplace = True)\n",
    "new_df.rename(columns = {0:'Full Account Name'}, inplace = True)\n",
    "new_df.rename(columns = {1:'Account Handle'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "f1809012-8a3f-4b16-ae70-8d73cd890b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines the dataframes to a new dataframe\n",
    "df2 = pd.concat([new_df, split_cols2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "d142d551-efcc-46d4-abc0-318b186d2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets the accounts that are verified with Verified \n",
    "df2['Verified Account'] = df2['Verified Account'].replace('','Verified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "246a3143-b95d-453a-ad67-5a1d1d4c2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets the accounts that are not verified with Not Verified\n",
    "df2['Verified Account'] = df2['Verified Account'].replace(to_replace=[None],value='Not Verified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "3e52d4ac-5ad3-4c10-88bb-2df82a231bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drops the Full Account Name column since we have that info split up\n",
    "df2 = df2.drop('Full Account Name', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "ddaa0e70-2d8c-4bb2-9090-b329fa4d3e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sets a new dataframe that reogranizes the df2 columns\n",
    "final_df = df2[['Account Name', 'Account Handle', 'Verified Account', 'Post Text', 'Post Grabbed Time', 'Author Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "4159f02c-0fa9-44ba-8752-041e51539671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account Name</th>\n",
       "      <th>Account Handle</th>\n",
       "      <th>Verified Account</th>\n",
       "      <th>Post Text</th>\n",
       "      <th>Post Grabbed Time</th>\n",
       "      <th>Author Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Luis Rosales</td>\n",
       "      <td>ese_cholito</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Bad Bunny, Taylor Swift and MORE are taken off...</td>\n",
       "      <td>2024-02-02 17:04:06.252082</td>\n",
       "      <td>Luis Rosales@ese_cholito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free_Talk</td>\n",
       "      <td>Free_Talk</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>This swift is a nobody, why on earth do people...</td>\n",
       "      <td>2024-02-02 17:04:06.252082</td>\n",
       "      <td>Free_Talk@Free_Talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barbara Campbell</td>\n",
       "      <td>dogmama53</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Proof Taylor Swift is a psyopMarilyn@Jaizemthi...</td>\n",
       "      <td>2024-02-02 17:04:06.252082</td>\n",
       "      <td>Barbara Campbell@dogmama53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Erich Brink</td>\n",
       "      <td>Indignatie</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>https://indignatie.nl/trump-bondgenoten-belove...</td>\n",
       "      <td>2024-02-02 17:04:06.252082</td>\n",
       "      <td>Erich Brink@Indignatie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>klsparrow</td>\n",
       "      <td>klsparrow</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Swift has not yet endorsed anyone in the 2024 ...</td>\n",
       "      <td>2024-02-02 17:04:06.252082</td>\n",
       "      <td>klsparrow@klsparrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>Augusto’s Helicopter Tours</td>\n",
       "      <td>LaResistance</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Taylor Swift is not a psyop ... I beg to disag...</td>\n",
       "      <td>2024-02-02 17:04:06.252082</td>\n",
       "      <td>Augusto’s Helicopter Tours@LaResistance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>Veritas Lux Mea</td>\n",
       "      <td>Veritas_Lux_Mea</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>https://nationalfile.com/mr-pfizer-travis-kelc...</td>\n",
       "      <td>2024-02-02 17:04:06.252082</td>\n",
       "      <td>Veritas Lux Mea@Veritas_Lux_Mea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1074</th>\n",
       "      <td>John Bernhard</td>\n",
       "      <td>PatriotJohnB52</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>https://nationalfile.com/mr-pfizer-travis-kelc...</td>\n",
       "      <td>2024-02-02 17:04:06.252082</td>\n",
       "      <td>John Bernhard@PatriotJohnB52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>Gabriel’s Horn</td>\n",
       "      <td>Gabriels_Horn</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>https://www.breitbart.com/politics/2024/02/01/...</td>\n",
       "      <td>2024-02-02 17:04:06.252082</td>\n",
       "      <td>Gabriel’s Horn@Gabriels_Horn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>The Unknown Cynic</td>\n",
       "      <td>TheUnknownCynic</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>It appears that Rep. Marjorie Taylor Greene@re...</td>\n",
       "      <td>2024-02-02 17:04:06.252082</td>\n",
       "      <td>The Unknown Cynic@TheUnknownCynic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Account Name   Account Handle Verified Account  \\\n",
       "0                   Luis Rosales      ese_cholito     Not Verified   \n",
       "2                      Free_Talk        Free_Talk     Not Verified   \n",
       "4               Barbara Campbell        dogmama53     Not Verified   \n",
       "6                    Erich Brink       Indignatie     Not Verified   \n",
       "10                     klsparrow        klsparrow     Not Verified   \n",
       "...                          ...              ...              ...   \n",
       "1070  Augusto’s Helicopter Tours     LaResistance     Not Verified   \n",
       "1072             Veritas Lux Mea  Veritas_Lux_Mea     Not Verified   \n",
       "1074               John Bernhard   PatriotJohnB52     Not Verified   \n",
       "1076              Gabriel’s Horn    Gabriels_Horn     Not Verified   \n",
       "1112           The Unknown Cynic  TheUnknownCynic     Not Verified   \n",
       "\n",
       "                                              Post Text  \\\n",
       "0     Bad Bunny, Taylor Swift and MORE are taken off...   \n",
       "2     This swift is a nobody, why on earth do people...   \n",
       "4     Proof Taylor Swift is a psyopMarilyn@Jaizemthi...   \n",
       "6     https://indignatie.nl/trump-bondgenoten-belove...   \n",
       "10    Swift has not yet endorsed anyone in the 2024 ...   \n",
       "...                                                 ...   \n",
       "1070  Taylor Swift is not a psyop ... I beg to disag...   \n",
       "1072  https://nationalfile.com/mr-pfizer-travis-kelc...   \n",
       "1074  https://nationalfile.com/mr-pfizer-travis-kelc...   \n",
       "1076  https://www.breitbart.com/politics/2024/02/01/...   \n",
       "1112  It appears that Rep. Marjorie Taylor Greene@re...   \n",
       "\n",
       "              Post Grabbed Time                              Author Name  \n",
       "0    2024-02-02 17:04:06.252082                 Luis Rosales@ese_cholito  \n",
       "2    2024-02-02 17:04:06.252082                      Free_Talk@Free_Talk  \n",
       "4    2024-02-02 17:04:06.252082               Barbara Campbell@dogmama53  \n",
       "6    2024-02-02 17:04:06.252082                   Erich Brink@Indignatie  \n",
       "10   2024-02-02 17:04:06.252082                      klsparrow@klsparrow  \n",
       "...                         ...                                      ...  \n",
       "1070 2024-02-02 17:04:06.252082  Augusto’s Helicopter Tours@LaResistance  \n",
       "1072 2024-02-02 17:04:06.252082          Veritas Lux Mea@Veritas_Lux_Mea  \n",
       "1074 2024-02-02 17:04:06.252082             John Bernhard@PatriotJohnB52  \n",
       "1076 2024-02-02 17:04:06.252082             Gabriel’s Horn@Gabriels_Horn  \n",
       "1112 2024-02-02 17:04:06.252082        The Unknown Cynic@TheUnknownCynic  \n",
       "\n",
       "[132 rows x 6 columns]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prints the final dataframe to make sure everything looks good\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "cb14d8fc-2ab2-45af-b55e-2c137ddaa1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exports the final version of the dataframe to CSV\n",
    "final_df.to_csv('GabPosts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6b1c5-e3b6-409f-81e4-2f42074b29d2",
   "metadata": {},
   "source": [
    "Post Assignment Thoughts:\n",
    "\n",
    "I initially wanted to crawl Truth Social, but I had trouble logging in. Gab was a good back up to crawl since it was easy to find the elements I needed and I did not need to make an account or anything. It was quite easy to access. \n",
    "\n",
    "An issue that isn't considered in this code is that many posts contained pictures which I did not get. This seems to be the primary way of sharing information on the site. If I was going to revisit this later on, I would want to add the pictures as well. \n",
    "\n",
    "As I mentioned above, I would ideally want this to run every 2 hours or so and then update the CSV with rows that would not be duplicates which is not included in this project. Although I consider this project to accomplish what is being asked for the assignment, there are still pieces to it that could be added. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
